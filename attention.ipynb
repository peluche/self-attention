{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1767YGfJBUJ"
      },
      "source": [
        "# self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6uEL9POJdJg"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "eZ2JLTWoI2Xc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "R5UzPzqsJ1DM"
      },
      "outputs": [],
      "source": [
        "EMBED_SIZE = 8\n",
        "VOCAB_SIZE = 11\n",
        "HIDDEN_SIZE = 32\n",
        "CONTEXT_SIZE = 10\n",
        "MAGIC_TOKEN = VOCAB_SIZE-1\n",
        "EPOCHS = 10000\n",
        "LEARNING_RATE = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "sLH6wwNFmoPp"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(1000):\n",
        "  magic_token_idx = random.randint(1, CONTEXT_SIZE/2 -1)\n",
        "  x = [random.randint(1,VOCAB_SIZE-2) for _ in range(magic_token_idx)] + [MAGIC_TOKEN] + [0 for _ in range(CONTEXT_SIZE - magic_token_idx - 1)]\n",
        "  y = x[:magic_token_idx+1] + x[:magic_token_idx] + [0 for _ in range(CONTEXT_SIZE - 2 * magic_token_idx - 1)]\n",
        "  X.append(x)\n",
        "  Y.append(y)\n",
        "\n",
        "X = torch.tensor(X).to(device)\n",
        "Y= torch.tensor(Y).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwuS4B5TJfVW"
      },
      "source": [
        "## code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "2svm5EyvNtMa"
      },
      "outputs": [],
      "source": [
        "def get_training():\n",
        "  X = torch.tensor([[0, 1, 2, 3],\n",
        "                    [3, 2, 1, 0]])\n",
        "\n",
        "  X = torch.randint(0, VOCAB_SIZE-2, (1000, CONTEXT_SIZE))\n",
        "\n",
        "  Y = torch.ones_like(X) # TODO\n",
        "  return X.to(device), Y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "NyzZx3uXJam-"
      },
      "outputs": [],
      "source": [
        "activations = {}\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.w_key = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "    self.w_query = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "    self.w_value = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "    self.ln = nn.LayerNorm(EMBED_SIZE)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # generate K,Q,V\n",
        "    key = self.w_key(x)\n",
        "    query = self.w_query(x)\n",
        "    value = self.w_value(x)\n",
        "    # pre-layernorm\n",
        "    # x = self.ln(x)\n",
        "    # do the attention\n",
        "    correlation = query @ key.transpose(-2, -1)\n",
        "    correlation = correlation / math.sqrt(key.shape[-1])\n",
        "    correlation = F.softmax(correlation, dim=-1)\n",
        "    activations['x'] = correlation.detach()\n",
        "    new_embedding = correlation @ value\n",
        "    # post-layernorm\n",
        "    new_embedding = self.ln(new_embedding)\n",
        "    return new_embedding\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
        "    self.positional_embedding = nn.Embedding(CONTEXT_SIZE, EMBED_SIZE)\n",
        "    self.attention = Attention()\n",
        "    self.ff = nn.Sequential(\n",
        "      nn.Linear(EMBED_SIZE, HIDDEN_SIZE),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(HIDDEN_SIZE, EMBED_SIZE),\n",
        "      nn.LayerNorm(EMBED_SIZE),\n",
        "    )\n",
        "    self.head = nn.Linear(EMBED_SIZE, VOCAB_SIZE)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch_size, context_size)\n",
        "    x = self.token_embedding(x)  # (batch_size, context_size, embedding_size)\n",
        "    # positional encoding\n",
        "    x = x + self.positional_embedding(torch.arange(0, x.shape[1]).to(device))\n",
        "    # attention\n",
        "    x = x + self.attention(x)\n",
        "    # feed forward\n",
        "    x = x + self.ff(x)\n",
        "    # head\n",
        "    x = self.head(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "qnggoXHwK7zo"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "gDVu722fp_PW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    0 0.023644037544727325\n",
            "  500 0.021885551512241364\n",
            " 1000 0.019897228106856346\n",
            " 1500 0.01824447140097618\n",
            " 2000 0.016821494325995445\n",
            " 2500 0.015355621464550495\n",
            " 3000 0.014725381508469582\n",
            " 3500 0.013623536564409733\n",
            " 4000 0.013357420451939106\n",
            " 4500 0.012485885992646217\n",
            " 5000 0.01240008044987917\n",
            " 5500 0.011182711459696293\n",
            " 6000 0.011294741183519363\n",
            " 6500 0.010275159031152725\n",
            " 7000 0.009729313664138317\n",
            " 7500 0.009509393014013767\n",
            " 8000 0.008743996731936932\n",
            " 8500 0.008169224485754967\n",
            " 9000 0.008465222083032131\n",
            " 9500 0.007913067936897278\n"
          ]
        }
      ],
      "source": [
        "def train(model, epochs=EPOCHS, lr=LEARNING_RATE):\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    out = model(X)\n",
        "    loss = F.cross_entropy(out.view(-1, out.shape[-1]), Y.view(-1))\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'{epoch:5} {loss.item()}')\n",
        "\n",
        "train(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xhSeSabbVMF",
        "outputId": "6838c255-35b8-4f0e-81da-8714c9e24216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.940000057220459\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 4, 10,  4,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 2,  2,  6, 10,  2,  2,  6,  0,  0,  0],\n",
              "        [ 6, 10,  6,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 4,  8, 10,  4,  8,  0,  0,  0,  0,  0],\n",
              "        [ 1,  5,  6, 10,  1,  5,  6,  0,  0,  0],\n",
              "        [ 5,  2,  7,  4, 10,  5,  2,  7,  4,  0],\n",
              "        [ 2,  7,  2,  3, 10,  2,  7,  2,  3,  0],\n",
              "        [ 4,  4,  7, 10,  4,  4,  7,  0,  0,  0],\n",
              "        [ 4,  4,  4, 10,  4,  4,  4,  0,  0,  0],\n",
              "        [ 1,  9,  6,  3, 10,  9,  8,  6,  3,  0],\n",
              "        [ 7, 10,  7,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 3,  5, 10,  3,  5,  0,  0,  0,  0,  0],\n",
              "        [ 1,  9,  5,  8, 10,  9,  9,  5,  8,  0],\n",
              "        [ 4, 10,  4,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 5,  4, 10,  5,  4,  0,  0,  0,  0,  0],\n",
              "        [ 7, 10,  7,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 2,  5, 10,  2,  5,  0,  0,  0,  0,  0],\n",
              "        [ 5,  1, 10,  5,  1,  0,  0,  0,  0,  0],\n",
              "        [ 1, 10,  1,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 9, 10,  9,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')"
            ]
          },
          "execution_count": 368,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = model(X).argmax(-1)\n",
        "correct = ((res == Y).float().sum(-1) == CONTEXT_SIZE).float().sum()\n",
        "print(f'accuracy: {correct / len(X)}')\n",
        "res[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 8,  3,  4, 10,  8,  3,  4,  0,  0,  0]], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1b11d64c090>"
            ]
          },
          "execution_count": 366,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWNklEQVR4nO3df4yUhb3v8e/uIrMrXbiIBeW6KPX2FgVUEDDKjW0j0WPU1JPG1gQTgontbRcFSUyhjXqNhZWmNeSKRTGtJan4I2mM1kQbz/YotZXwS4ymrbTHE7vVA+gt2VWoC+7M/aPttpwRzw7sl2cGX69k/uDJDM8nw8KbZwdmmiqVSiUAYJg1Fz0AgOOTwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0CKEcf6hOVyOd56661ob2+PpqamY316AI5CpVKJd999NyZOnBjNzR99jXLMA/PWW29FR0fHsT4tAMOop6cnTjvttI+8zzEPTHt7e0REXFz65xjRdMKxPv1hld/vL3pClaaZZxU9oUrlpd8WPaFK04j6+Tr6m8rBA0VPqPK/d+wsekKV+877n0VPqFKZM63oCVWatv6m6AmDPqgcjBcqPx38s/yjHPPA/O3bYiOaTogRTSOP9ekPq9xULnpClaaW1qInVKnU0V8K/qapDjdVmurvLf5ObG8pekKVevpL5t9URtTf77u6+xqvxJBe4vAiPwApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0CKIwrMvffeG2eccUa0trbGBRdcEJs3bx7uXQA0uJoD8+ijj8bSpUvj9ttvj+3bt8e5554bl112WezZsydjHwANqubA3H333XHDDTfEwoUL4+yzz4777rsvTjzxxPjhD3+YsQ+ABlVTYA4cOBDbtm2LefPm/f0naG6OefPmxYsvvvihj+nv74++vr5DbgAc/2oKzDvvvBMDAwMxYcKEQ45PmDAhdu3a9aGP6erqijFjxgzefJolwMdD+r8iW758efT29g7eenp6sk8JQB2o6RMtTz755GhpaYndu3cfcnz37t1xyimnfOhjSqVSlEqlI18IQEOq6Qpm5MiRcf7550d3d/fgsXK5HN3d3XHhhRcO+zgAGldNVzAREUuXLo0FCxbErFmzYs6cObF69erYt29fLFy4MGMfAA2q5sB8+ctfjrfffjtuu+222LVrV5x33nnxzDPPVL3wD8DHW82BiYhYtGhRLFq0aLi3AHAc8V5kAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACmO6L3IhkO5/0CUmypFnb7Kz97aUfSEKpdNLHpBtRGn/feiJ1Qp/2lv0ROqNH9qUtETqvzf/1H0gg/R3FL0gipNL75c9IQqza2tRU8Y1FypRLw/xPvmTgHg40pgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFKMKOrE/fNmxMAJrUWdvsplEytFT6hy2at9RU+o8i+fPanoCVWaSqWiJ1QZ2PlvRU+o0jxqVNETqpT37y96QpURp55S9IQqH+zaXfSEQeXKwSHf1xUMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASFFTYLq6umL27NnR3t4e48ePj6uvvjpee+21rG0ANLCaAvP8889HZ2dnbNq0KZ599tk4ePBgXHrppbFv376sfQA0qJo+cOyZZ5455Mc/+tGPYvz48bFt27a4+OKLh3UYAI3tqD7Rsre3NyIiTjrp8J9y2N/fH/39/YM/7uurv09pBGD4HfGL/OVyOZYsWRJz586NadOmHfZ+XV1dMWbMmMFbR0fHkZ4SgAZyxIHp7OyMV199NR555JGPvN/y5cujt7d38NbT03OkpwSggRzRt8gWLVoUTz31VGzcuDFOO+20j7xvqVSKUql0ROMAaFw1BaZSqcSNN94Yjz/+eDz33HMxefLkrF0ANLiaAtPZ2RkbNmyIJ554Itrb22PXrl0RETFmzJhoa2tLGQhAY6rpNZi1a9dGb29vfO5zn4tTTz118Pboo49m7QOgQdX8LTIAGArvRQZACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQ4qg+MvlojOw7ECNG1FHf5kwvekGV7n/6U9ETqjS11t/70VXef7/oCVX6rr2g6AlVxv789aInVBkxur3oCVX2n/PRn3FVhBPbWoue8Hfl/oh/H9pd6+hPeACOJwIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkGJEUSf+4BMnRIw4oajTV2l75Y9FT6gy8PY7RU+o8qcnJhc9ocpJ/2dc0ROqjN1ef792A7v3FD2hStOIwv4IOqxSdx3+2hU94B8MVA4O+b6uYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0CKowrMXXfdFU1NTbFkyZJhmgPA8eKIA7Nly5a4//7745xzzhnOPQAcJ44oMO+9917Mnz8/HnjggRg7duxwbwLgOHBEgens7Iwrrrgi5s2b91/et7+/P/r6+g65AXD8q/nzSh955JHYvn17bNmyZUj37+rqijvuuKPmYQA0tpquYHp6emLx4sXx0EMPRWtr65Aes3z58ujt7R289fT0HNFQABpLTVcw27Ztiz179sTMmTMHjw0MDMTGjRtjzZo10d/fHy0tLYc8plQqRalUGp61ADSMmgJzySWXxCuvvHLIsYULF8aUKVPiG9/4RlVcAPj4qikw7e3tMW3atEOOjRo1KsaNG1d1HICPN/+TH4AUNf8rsv/sueeeG4YZABxvXMEAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApDjq9yI7Uq3//qcY0VI/nxNTGf2JoidUqccPP+jbdnLRE6q0fLpS9IQqY7fuL3pCtaamohdUaT7xxKInNIY6+iiU5sqBiL1DvG/uFAA+rgQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIMWIok7ctO/P0dQ8UNTpq3ywe0/RE6rs/+c5RU+o8sGoStETqrw3sf7+njR6578VPaFKU6lU9IQq5T+/X/SEKpUPDhY9oUpTS0vREwaVK0N/furvdyYAxwWBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUtQcmDfffDOuu+66GDduXLS1tcX06dNj69atGdsAaGA1fR7M3r17Y+7cufH5z38+nn766fjkJz8Zv/vd72Ls2LFZ+wBoUDUFZtWqVdHR0REPPvjg4LHJkycP+ygAGl9N3yJ78sknY9asWXHNNdfE+PHjY8aMGfHAAw985GP6+/ujr6/vkBsAx7+aAvP666/H2rVr49Of/nT87Gc/i6997Wtx0003xfr16w/7mK6urhgzZszgraOj46hHA1D/agpMuVyOmTNnxsqVK2PGjBnxla98JW644Ya47777DvuY5cuXR29v7+Ctp6fnqEcDUP9qCsypp54aZ5999iHHzjrrrPjDH/5w2MeUSqUYPXr0ITcAjn81BWbu3Lnx2muvHXJs586dcfrppw/rKAAaX02Bufnmm2PTpk2xcuXK+P3vfx8bNmyIdevWRWdnZ9Y+ABpUTYGZPXt2PP744/Hwww/HtGnT4s4774zVq1fH/Pnzs/YB0KBq+n8wERFXXnllXHnllRlbADiOeC8yAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQ1vxfZ8J25JaK5uNNXqVSKXlCl6at7ip5Q5TNfHSh6QpXKqLaiJ1Spv2cp4uD/mlb0hCoj/nVH0ROqtJx8ctETqpT37i16whFxBQNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASDGiqBNXRn8iKi2lok5fpXnvqKInVBn1hf8oekKV339rRtETqpz0m0rRE6qMjc8UPaFa97aiF1RpOn9q0ROqDLz026InVGkZd1LREwY1lw9EvDPE++ZOAeDjSmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUNQVmYGAgbr311pg8eXK0tbXFmWeeGXfeeWdUKvX3dukAFKumz4NZtWpVrF27NtavXx9Tp06NrVu3xsKFC2PMmDFx0003ZW0EoAHVFJhf/epX8YUvfCGuuOKKiIg444wz4uGHH47NmzenjAOgcdX0LbKLLroouru7Y+fOnRER8fLLL8cLL7wQl19++WEf09/fH319fYfcADj+1XQFs2zZsujr64spU6ZES0tLDAwMxIoVK2L+/PmHfUxXV1fccccdRz0UgMZS0xXMY489Fg899FBs2LAhtm/fHuvXr4/vfve7sX79+sM+Zvny5dHb2zt46+npOerRANS/mq5gbrnllli2bFlce+21ERExffr0eOONN6KrqysWLFjwoY8plUpRKpWOfikADaWmK5j9+/dHc/OhD2lpaYlyuTysowBofDVdwVx11VWxYsWKmDRpUkydOjVeeumluPvuu+P666/P2gdAg6opMPfcc0/ceuut8fWvfz327NkTEydOjK9+9atx2223Ze0DoEHVFJj29vZYvXp1rF69OmkOAMcL70UGQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkKKm9yIbVv+xO6JpZGGn/8/K+/cXPaHKgUvPL3pClU+teKnoCVWa/9uYoidUGfh/e4ueUGXn/bOLnlDlMw/8uegJVUac0VH0hGqVStELBjWVWyLeGdp9XcEAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBhxrE9YqVQiIuKDyoFjfeqPNFA5WPSEKh988H7RE6rU269bRERzuf421ePXU/nPdfj1NFB/myrl/qInVPvrn5v14IO//n6rDGFTU2Uo9xpGf/zjH6Ojo+NYnhKAYdbT0xOnnXbaR97nmAemXC7HW2+9Fe3t7dHU1HTEP09fX190dHRET09PjB49ehgXHl88T0PjeRoaz9PQHM/PU6VSiXfffTcmTpwYzc0f/SrLMf8WWXNz839ZvVqMHj36uPsFzOB5GhrP09B4nobmeH2exowZM6T7eZEfgBQCA0CKhg1MqVSK22+/PUqlUtFT6prnaWg8T0PjeRoaz9NfHPMX+QH4eGjYKxgA6pvAAJBCYABIITAApGjYwNx7771xxhlnRGtra1xwwQWxefPmoifVla6urpg9e3a0t7fH+PHj4+qrr47XXnut6Fl17a677oqmpqZYsmRJ0VPqzptvvhnXXXddjBs3Ltra2mL69OmxdevWomfVlYGBgbj11ltj8uTJ0dbWFmeeeWbceeedQ3rPruNVQwbm0UcfjaVLl8btt98e27dvj3PPPTcuu+yy2LNnT9HT6sbzzz8fnZ2dsWnTpnj22Wfj4MGDcemll8a+ffuKnlaXtmzZEvfff3+cc845RU+pO3v37o25c+fGCSecEE8//XT8+te/ju9973sxduzYoqfVlVWrVsXatWtjzZo18Zvf/CZWrVoV3/nOd+Kee+4pelphGvKfKV9wwQUxe/bsWLNmTUT85f3NOjo64sYbb4xly5YVvK4+vf322zF+/Ph4/vnn4+KLLy56Tl157733YubMmfH9738/vv3tb8d5550Xq1evLnpW3Vi2bFn88pe/jF/84hdFT6lrV155ZUyYMCF+8IMfDB774he/GG1tbfHjH/+4wGXFabgrmAMHDsS2bdti3rx5g8eam5tj3rx58eKLLxa4rL719vZGRMRJJ51U8JL609nZGVdcccUhX1P83ZNPPhmzZs2Ka665JsaPHx8zZsyIBx54oOhZdeeiiy6K7u7u2LlzZ0REvPzyy/HCCy/E5ZdfXvCy4hzzN7s8Wu+8804MDAzEhAkTDjk+YcKE+O1vf1vQqvpWLpdjyZIlMXfu3Jg2bVrRc+rKI488Etu3b48tW7YUPaVuvf7667F27dpYunRpfPOb34wtW7bETTfdFCNHjowFCxYUPa9uLFu2LPr6+mLKlCnR0tISAwMDsWLFipg/f37R0wrTcIGhdp2dnfHqq6/GCy+8UPSUutLT0xOLFy+OZ599NlpbW4ueU7fK5XLMmjUrVq5cGRERM2bMiFdffTXuu+8+gfkHjz32WDz00EOxYcOGmDp1auzYsSOWLFkSEydO/Ng+Tw0XmJNPPjlaWlpi9+7dhxzfvXt3nHLKKQWtql+LFi2Kp556KjZu3DisH5NwPNi2bVvs2bMnZs6cOXhsYGAgNm7cGGvWrIn+/v5oaWkpcGF9OPXUU+Pss88+5NhZZ50VP/nJTwpaVJ9uueWWWLZsWVx77bURETF9+vR44403oqur62MbmIZ7DWbkyJFx/vnnR3d39+Cxcrkc3d3dceGFFxa4rL5UKpVYtGhRPP744/Hzn/88Jk+eXPSkunPJJZfEK6+8Ejt27Bi8zZo1K+bPnx87duwQl7+aO3du1T9x37lzZ5x++ukFLapP+/fvr/oArpaWliiXywUtKl7DXcFERCxdujQWLFgQs2bNijlz5sTq1atj3759sXDhwqKn1Y3Ozs7YsGFDPPHEE9He3h67du2KiL98UFBbW1vB6+pDe3t71WtSo0aNinHjxnmt6h/cfPPNcdFFF8XKlSvjS1/6UmzevDnWrVsX69atK3paXbnqqqtixYoVMWnSpJg6dWq89NJLcffdd8f1119f9LTiVBrUPffcU5k0aVJl5MiRlTlz5lQ2bdpU9KS6EhEfenvwwQeLnlbXPvvZz1YWL15c9Iy689Of/rQybdq0SqlUqkyZMqWybt26oifVnb6+vsrixYsrkyZNqrS2tlY+9alPVb71rW9V+vv7i55WmIb8fzAA1L+Gew0GgMYgMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAAp/j99keCBSINujwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# res = model(torch.tensor([[7,1,9,2,10,0,0,0,0,0]]).to(device)).argmax(-1)\n",
        "# res = model(torch.tensor([[1,2,10,0,0,0,0,0,0,0]]).to(device)).argmax(-1)\n",
        "# res = model(torch.tensor([[8,10,0,0,0,0,0,0,0,0]]).to(device)).argmax(-1)\n",
        "res = model(torch.tensor([[8,3,4,10,0,0,0,0,0,0]]).to(device)).argmax(-1)\n",
        "print(res)\n",
        "\n",
        "plt.imshow(activations['x'].squeeze().cpu())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv_hf_nlp",
      "language": "python",
      "name": "venv_hf_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
