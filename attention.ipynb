{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1767YGfJBUJ"
      },
      "source": [
        "# self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6uEL9POJdJg"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eZ2JLTWoI2Xc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R5UzPzqsJ1DM"
      },
      "outputs": [],
      "source": [
        "EMBED_SIZE = 8\n",
        "VOCAB_SIZE = 11\n",
        "HIDDEN_SIZE = 320\n",
        "CONTEXT_SIZE = 10\n",
        "MAGIC_TOKEN = VOCAB_SIZE-1\n",
        "MAX_ITERS = 10000\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sLH6wwNFmoPp"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(1000):\n",
        "  magic_token_idx = random.randint(1, CONTEXT_SIZE/2 -1)\n",
        "  x = [random.randint(1,VOCAB_SIZE-2) for _ in range(magic_token_idx)] + [MAGIC_TOKEN] + [0 for _ in range(CONTEXT_SIZE-magic_token_idx -1)]\n",
        "  y = x[:magic_token_idx+1] + x[:magic_token_idx] + [0 for _ in range(CONTEXT_SIZE - 2*magic_token_idx -1)]\n",
        "  X.append(x)\n",
        "  Y.append(y)\n",
        "\n",
        "X = torch.tensor(X).to(device)\n",
        "Y= torch.tensor(Y).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwuS4B5TJfVW"
      },
      "source": [
        "## code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2svm5EyvNtMa"
      },
      "outputs": [],
      "source": [
        "def get_training():\n",
        "  X = torch.tensor([[0, 1, 2, 3],\n",
        "                    [3, 2, 1, 0]])\n",
        "\n",
        "  X = torch.randint(0, VOCAB_SIZE-2, (1000, CONTEXT_SIZE))\n",
        "\n",
        "  Y = torch.ones_like(X) # TODO\n",
        "  return X.to(device), Y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NyzZx3uXJam-"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding = torch.nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
        "\n",
        "    self.positional_embedding = torch.nn.Embedding(CONTEXT_SIZE, EMBED_SIZE)\n",
        "\n",
        "    self.w_key = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "    self.w_query = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "    self.w_value = torch.nn.Linear(EMBED_SIZE, EMBED_SIZE, bias=False)\n",
        "\n",
        "    self.emb_ln = nn.LayerNorm(EMBED_SIZE)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "      nn.Linear(EMBED_SIZE, HIDDEN_SIZE),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(HIDDEN_SIZE, EMBED_SIZE),\n",
        "      nn.LayerNorm(EMBED_SIZE),\n",
        "    )\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(EMBED_SIZE, VOCAB_SIZE),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch_size, context_size)\n",
        "    x = self.token_embedding(x)  # (batch_size, context_size, embedding_size)\n",
        "    # positional embedding\n",
        "    x = x + self.positional_embedding(torch.arange(0, x.shape[1]).to(device))  # (batch_size, context_size, embedding_size)\n",
        "\n",
        "    # generate K,Q,V\n",
        "    key = self.w_key(x) #(batch_size, context_size, embedding_size) @ (embedding_size, embedding_size) ---> (batch_size, context_size, embedding_size)\n",
        "    query = self.w_query(x)\n",
        "    value = self.w_value(x)\n",
        "    # do the attention\n",
        "    correlation = query @ key.transpose(-2, -1)\n",
        "    correlation = correlation / math.sqrt(key.shape[-1])\n",
        "    new_embedding = correlation.softmax(-1) @ value\n",
        "\n",
        "    x = x + self.emb_ln(new_embedding)\n",
        "\n",
        "    # layernorm + MLP\n",
        "    fed = self.ff(x)\n",
        "    x = x + fed\n",
        "    x = self.layers(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qnggoXHwK7zo"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "# X, Y = get_training()\n",
        "# model(X).argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gDVu722fp_PW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    0 2.3383076190948486\n",
            "  500 1.1538957357406616\n",
            " 1000 1.0742956399917603\n",
            " 1500 1.0440741777420044\n",
            " 2000 1.0273371934890747\n",
            " 2500 1.0176148414611816\n",
            " 3000 1.0117547512054443\n",
            " 3500 1.0040936470031738\n",
            " 4000 0.9944692850112915\n",
            " 4500 0.9918068051338196\n",
            " 5000 0.9960141777992249\n",
            " 5500 0.9873514771461487\n",
            " 6000 0.9877238869667053\n",
            " 6500 0.9844509959220886\n",
            " 7000 0.9845349192619324\n",
            " 7500 0.9810346961021423\n",
            " 8000 0.980131208896637\n",
            " 8500 0.9796953201293945\n",
            " 9000 0.9905064105987549\n",
            " 9500 0.976533830165863\n"
          ]
        }
      ],
      "source": [
        "### train\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(MAX_ITERS):\n",
        "  out = model(X)\n",
        "  loss = F.cross_entropy(out, F.one_hot(Y, VOCAB_SIZE).float())\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "  if epoch % 500 == 0:\n",
        "    print(f'{epoch:5} {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xhSeSabbVMF",
        "outputId": "6838c255-35b8-4f0e-81da-8714c9e24216"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[9, 0, 0, 10, 1, 0, 1, 0, 0, 0],\n",
              " [0, 6, 0, 0, 0, 0, 6, 0, 0, 0],\n",
              " [0, 0, 0, 10, 1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0, 6, 0, 0, 0],\n",
              " [6, 0, 10, 1, 1, 0, 0, 0, 0, 0],\n",
              " [0, 10, 6, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 10, 6, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 1, 6, 0, 0, 0],\n",
              " [0, 9, 8, 1, 0, 1, 6, 0, 0, 0],\n",
              " [9, 0, 10, 1, 4, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(X).argmax(-1).tolist()[:10]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv_hf_nlp",
      "language": "python",
      "name": "venv_hf_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
